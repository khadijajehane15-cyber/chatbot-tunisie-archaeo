import chromadb
from sentence_transformers import SentenceTransformer
import ollama
import time

print("=" * 60)
print("ğŸ¤– CHATBOT RAG - Gemma3:1b (LÃ©ger et rapide)")
print("=" * 60)

# Initialisation
print("\nğŸ”§ Initialisation...")

# 1. ModÃ¨le d'embedding
print("ğŸ“Š Chargement du modÃ¨le d'embedding...")
embedding_model = SentenceTransformer('all-MiniLM-L6-v2')

# 2. Connexion Ã  ChromaDB
print("ğŸ’¾ Connexion Ã  la base de donnÃ©es...")
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_collection("sites_archeo_tunisie")

# 3. VÃ©rification du modÃ¨le
print("ğŸ§  VÃ©rification du modÃ¨le Gemma3:1b...")
try:
    # Tester avec un petit prompt
    test_response = ollama.chat(
        model='gemma3:1b',
        messages=[{'role': 'user', 'content': 'Test'}],
        options={'num_gpu': 1, 'num_thread': 2}  # Utilise un peu de GPU
    )
    print("âœ… Gemma3:1b fonctionne !")
    print(f"   Test: {test_response['message']['content'][:50]}...")
except Exception as e:
    print(f"âš ï¸  Erreur: {str(e)[:100]}")
    print("ğŸ”§ Passage en mode CPU...")

print("\n" + "âœ… SYSTÃˆME PRÃŠT !".center(60))
print("-" * 60)

def recherche_documents(question, top_k=2):
    """Recherche les documents pertinents"""
    question_embedding = embedding_model.encode(question).tolist()
    
    results = collection.query(
        query_embeddings=[question_embedding],
        n_results=top_k
    )
    
    return results

def generer_reponse_gemma(question, documents, metadatas):
    """GÃ©nÃ¨re une rÃ©ponse avec Gemma3:1b"""
    # Contexte simple
    contexte = ""
    for i, (doc, meta) in enumerate(zip(documents, metadatas)):
        contexte += f"[Source {i+1}: {meta['site']}]\n{doc[:500]}\n\n"
    
    # Prompt court (important pour modÃ¨le lÃ©ger)
    prompt = f"""Contexte:
{contexte}

Question: {question}

RÃ©ponds briÃ¨vement en franÃ§ais avec les informations ci-dessus. Cite [Source X]."""

    try:
        start_time = time.time()
        
        response = ollama.chat(
            model='gemma3:1b',
            messages=[{'role': 'user', 'content': prompt}],
            options={
                'num_gpu': 1,      # Essaie avec un peu de GPU
                'num_thread': 2,   # Peu de threads
                'temperature': 0.1 # TrÃ¨s factuel
            }
        )
        
        end_time = time.time()
        print(f"   â±ï¸  Temps de rÃ©ponse: {end_time - start_time:.1f}s")
        
        return response['message']['content']
        
    except Exception as e:
        # Fallback en mode CPU
        print(f"   âš ï¸  Essai en mode CPU...")
        try:
            response = ollama.chat(
                model='gemma3:1b',
                messages=[{'role': 'user', 'content': prompt}],
                options={'num_gpu': 0, 'num_thread': 2}
            )
            return response['message']['content']
        except:
            # RÃ©ponse trÃ¨s simple
            if documents:
                return f"D'aprÃ¨s {metadatas[0]['site']} : {documents[0][:150]}..."
            return "Information non disponible."

def chatbot_principal():
    """Boucle principale"""
    print("\nğŸ’¬ Pose des questions sur Carthage, Dougga, El Jem, Kerkouane")
    print("   'quit'=quitter, 'sites'=liste")
    print("-" * 60)
    
    while True:
        question = input("\nğŸ‘‰ Question : ").strip()
        
        if question.lower() == 'quit':
            print("\nğŸ‘‹ Au revoir !")
            break
            
        elif question.lower() == 'sites':
            print("\nğŸ“š SITES:")
            print("   â€¢ Carthage - Banlieue de Tunis")
            print("   â€¢ Dougga - Nord-ouest (BÃ©ja)")
            print("   â€¢ El Jem - Centre-est (Mahdia)")
            print("   â€¢ Kerkouane - Cap Bon")
            continue
        
        if not question:
            continue
        
        print("ğŸ” Recherche...", end='', flush=True)
        
        # Recherche
        results = recherche_documents(question)
        
        if not results['documents'][0]:
            print("\nâŒ Aucun document trouvÃ©.")
            continue
        
        print("âœ“")
        print("ğŸ§  GÃ©nÃ©ration...", end='', flush=True)
        
        # GÃ©nÃ©ration
        documents = results['documents'][0]
        metadatas = results['metadatas'][0]
        
        reponse = generer_reponse_gemma(question, documents, metadatas)
        
        print("âœ“")
        
        # Affichage
        print("\n" + "ğŸ“‹ RÃ‰PONSE " + "â”€" * 46)
        print(reponse)
        print("â”€" * 60)
        
        print("ğŸ“š SOURCES:")
        for i, meta in enumerate(metadatas):
            print(f"   {i+1}. {meta['site']}")

if __name__ == "__main__":
    chatbot_principal()